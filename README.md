# RAG System with Local LLM (Llama3)
Hi there! I'm excited that you've found this project! This implementation of a Retrieval-Augmented Generation (RAG) LLM is designed to make it easy for you to upload and analyze an unlimited number of documents using a chatbot. It's a simple, yet powerful solution to enhance document-based queries. Have fun!


This project implements a **Retrieval-Augmented Generation (RAG)** system using a local language model (Llama3) to retrieve relevant documents and answer queries. The setup leverages **LangChain**, **FAISS** for vector storage, and **Streamlit** for the UI.

## Features

- **Document Retrieval**: Automatically loads PDF documentss from a specified folder.
- **Local LLM**: Uses the Ollama Llama3 model for generating responses.
- **Interactive UI**: Built with Streamlit, allowing users to input prompts and see responses in real-time.

## Prerequisites

To run this project, you need the following installed:

- Python 3.8 or higher
- **Ollama**: Local LLM management tool
- **Streamlit**: For the UI interface
- **LangChain**: For chaining LLM and document retrieval functionalities

### Installation

1. **Install Python packages**:
   - First, navigate into the project directory and clone the repository:
     ```bash
     cd <your-project-directory>
     git clone https://github.com/dorawengg/RAG-LLM.git
     ```
   - Install the required Python packages using `pip`:
     ```bash
     pip install langchain streamlit langchain_community langchain_huggingface faiss-cpu
     ```

2. **Install Ollama**:
   - Install Ollama following the official installation guide:
     - For MacOS, use the following command:
       ```bash
       brew install ollama
       ```
     - For other platforms, visit [Ollama's official site](https://ollama.com).

3. **Download the Llama3 model**:
   - After installing Ollama, download the Llama3 model:
     ```bash
     ollama pull llama3
     ollama run llama3
     ```

## Running the App

1. **Prepare the Document Folder**:
   - Add your PDF documents to the `./docs` folder. These documents will be automatically loaded by the system.

2. **Run the Streamlit app**:
   - Once everything is set up, you can start the app by running:
     ```bash
     streamlit run remoterag.py
     ```
   - This will launch the Streamlit app in your default browser, where you can input a prompt and see how the local LLM (Llama3) responds based on the context of the loaded documents.

## How It Works

1. **Document Loading**:
   - The app scans the `./docs` directory and loads any PDF documents it finds using LangChain's `PyPDFLoader`.
   
2. **FAISS Indexing**:
   - The content of the PDFs is split into chunks and indexed using **FAISS** for efficient vector-based retrieval.

3. **Querying the LLM**:
   - When you enter a prompt, the system retrieves the most relevant chunks of the documents and passes them as context to the Llama3 model, which generates a response.

## Example

1. Place your PDF documents inside the `./docs` directory (e.g., `sample.pdf`).
2. Open the Streamlit app and enter a query prompt like "What are the key takeaways from the document?"
3. The system retrieves the most relevant document sections and provides an answer generated by the Llama3 model.

## Troubleshooting

- **Ollama model not loading**: Ensure you have successfully installed Ollama and downloaded the Llama3 model by running `ollama pull llama3`.
- **Streamlit not loading**: Ensure all required packages are installed and run the app using `streamlit run remoterag.py`.

## Future Improvements

- Support for more document formats (e.g., DOCX, TXT)
- Integration with external APIs for internet-based queries
